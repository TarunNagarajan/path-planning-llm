{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nSw2CjKQ5lSq"
      },
      "outputs": [],
      "source": [
        "!pip install cupy-cuda11x"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc --version"
      ],
      "metadata": {
        "id": "GjDhpBXT6WjY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y cupy cupy-cuda12x"
      ],
      "metadata": {
        "id": "Mo_GqMEI7Frj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get update -y\n",
        "!apt-get install -y cuda-toolkit-11-8"
      ],
      "metadata": {
        "id": "tO2zkMdv7LSh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install cupy-cuda11x"
      ],
      "metadata": {
        "id": "tyMwJ4Dg7Qn3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc --version"
      ],
      "metadata": {
        "id": "oanVRE9C8sFQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U cupy-cuda12x"
      ],
      "metadata": {
        "id": "V2CdFHCl83fS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc --version\n"
      ],
      "metadata": {
        "id": "UjnKjXm689Yo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get remove --purge '^cuda.*' -y\n",
        "!apt-get remove --purge '^nvidia-.*' -y\n",
        "!apt-get autoremove -y\n",
        "!apt-get clean\n",
        "!pip uninstall -y cupy cupy-cuda*\n"
      ],
      "metadata": {
        "id": "1JSg9Igt9SpK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install cupy-cuda12x==13.1.0\n"
      ],
      "metadata": {
        "id": "-kC2DtbB-8CT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cupy as cp\n",
        "print(cp.cuda.is_available())\n",
        "gpu_array = cp.array([1, 2, 3, 4, 5])\n",
        "print(gpu_array)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rjNdMjSi_AVp",
        "outputId": "791d43ad-6bb3-4eb3-ec42-65bd88329e7c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "[1 2 3 4 5]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = cp.array([1, 2, 3])\n",
        "b = cp.array([4, 5, 6])\n",
        "\n",
        "print(a + b)\n",
        "print(a * b)\n",
        "print(cp.exp(a))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CCDDHiqx_bcr",
        "outputId": "a4b897d0-456b-48aa-a4bb-279ef748ceee"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5 7 9]\n",
            "[ 4 10 18]\n",
            "[ 2.71828183  7.3890561  20.08553692]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "cpu_array = np.array([1, 2, 3])\n",
        "gpu_array = cp.array(cpu_array)\n",
        "cpu_result = cp.asnumpy(gpu_array)\n",
        "\n",
        "print(\"Numpy (CPU) Array:\", cpu_array)\n",
        "print(\"CuPy (GPU) Array:\", gpu_array)\n",
        "print(\"Converted:\", cpu_result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tNWskZS0_wap",
        "outputId": "78a95a70-178c-4282-b041-347c60d01272"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Numpy (CPU) Array: [1 2 3]\n",
            "CuPy (GPU) Array: [1 2 3]\n",
            "Converted: [1 2 3]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cupy as cp\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "# Create a large NumPy array (100 million elements)\n",
        "cpu_array = np.random.rand(100_000_000)\n",
        "\n",
        "# Move to GPU\n",
        "start_gpu = time.time()\n",
        "gpu_array = cp.array(cpu_array)  # Transfer CPU -> GPU\n",
        "end_gpu = time.time()\n",
        "\n",
        "# Move back to CPU\n",
        "start_cpu = time.time()\n",
        "cpu_result = cp.asnumpy(gpu_array)  # Transfer GPU -> CPU\n",
        "end_cpu = time.time()\n",
        "\n",
        "print(f\"CPU -> GPU Transfer Time: {end_gpu - start_gpu:.6f} sec\")\n",
        "print(f\"GPU -> CPU Transfer Time: {end_cpu - start_cpu:.6f} sec\")\n",
        "\n",
        "# Measure repeated transfers\n",
        "repeats = 10\n",
        "start_repeat = time.time()\n",
        "for _ in range(repeats):\n",
        "    gpu_array = cp.array(cpu_array)  # CPU -> GPU\n",
        "    cpu_result = cp.asnumpy(gpu_array)  # GPU -> CPU\n",
        "end_repeat = time.time()\n",
        "\n",
        "print(f\"Average Transfer Time (CPU ↔ GPU, {repeats} times): {(end_repeat - start_repeat) / repeats:.6f} sec\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nXRy6PjaAKrd",
        "outputId": "a11e2dbe-9097-4b3f-a433-5188528fc37e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU -> GPU Transfer Time: 0.839782 sec\n",
            "GPU -> CPU Transfer Time: 0.362003 sec\n",
            "Average Transfer Time (CPU ↔ GPU, 10 times): 0.514063 sec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Matrix Multiplication on GPU"
      ],
      "metadata": {
        "id": "NrcWCKSPBoWM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import cupy as cp\n",
        "import time\n",
        "\n",
        "N = 4096\n",
        "\n",
        "A_cpu = np.random.rand(N, N)\n",
        "B_cpu = np.random.rand(N, N)\n",
        "\n",
        "start_cpu = time.time()\n",
        "C_cpu = np.dot(A_cpu, B_cpu)\n",
        "end_cpu = time.time()\n",
        "\n",
        "A_gpu = cp.asarray(A_cpu)\n",
        "B_gpu = cp.asarray(B_cpu)\n",
        "start_gpu = time.time()\n",
        "C_gpu = cp.dot(A_gpu, B_gpu)\n",
        "cp.cuda.Device(0).synchronize()\n",
        "end_gpu = time.time()\n",
        "\n",
        "# Compare Execution Times\n",
        "print(f\"CPU Time: {end_cpu - start_cpu:.6f} sec\")\n",
        "print(f\"GPU Time: {end_gpu - start_gpu:.6f} sec\")\n",
        "\n",
        "# Verify Correctness\n",
        "C_check = cp.asnumpy(C_gpu)  # Transfer result back to CPU\n",
        "print(f\"Error between CPU and GPU results: {np.max(np.abs(C_check - C_cpu))}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7dm0G1mKBm1D",
        "outputId": "8e9852aa-043f-4f9a-86f7-37988a6d13e9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU Time: 3.395280 sec\n",
            "GPU Time: 0.768896 sec\n",
            "Error between CPU and GPU results: 8.640199666842818e-12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 10\n",
        "A_batch = cp.random.rand(batch_size, N, N)\n",
        "B_batch = cp.random.rand(batch_size, N, N)\n",
        "\n",
        "start_batched = time.time()\n",
        "C_batch = cp.matmul(A_batch, B_batch)\n",
        "cp.cuda.Device(0).synchronize()\n",
        "end_batched = time.time()\n",
        "\n",
        "print(f\"Batched GPU time: {end_batched - start_batched:.6f} sec\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YIXIz7vTCv3r",
        "outputId": "4eec1ff2-ad42-402a-de40-5b8787eb2958"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batched GPU time: 5.430521 sec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cupy as cp\n",
        "free_mem, total_mem = cp.cuda.Device(0).mem_info\n",
        "print(f\"Free memory: {free_mem / 1024**2:.2f} MB\")\n",
        "print(f\"Total memory: {total_mem / 1024**2:.2f} MB\")\n",
        "\n",
        "\n",
        "square_kernel = cp.RawKernel(r'''\n",
        "extern \"C\" __global__\n",
        "void square(const float* x, float* y, int n) {\n",
        "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    if (tid < n) {\n",
        "        y[tid] = x[tid] * x[tid];\n",
        "    }\n",
        "}\n",
        "''', 'square')\n",
        "\n",
        "# Create data on GPU\n",
        "n_elements = 100000\n",
        "x_gpu = cp.random.rand(n_elements, dtype=cp.float32)\n",
        "y_gpu = cp.zeros(n_elements, dtype=cp.float32)\n",
        "\n",
        "# Run the kernel: using 256 threads per block\n",
        "threads_per_block = 256\n",
        "blocks = (n_elements + threads_per_block - 1) // threads_per_block\n",
        "square_kernel((blocks,), (threads_per_block,), (x_gpu, y_gpu, n_elements))\n",
        "\n",
        "print(\"First 10 squared values:\", y_gpu[:10])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1otG1urmDaDd",
        "outputId": "c903ff3c-1304-4944-d09d-c32a8a026fa3"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Free memory: 14596.12 MB\n",
            "Total memory: 15095.06 MB\n",
            "First 10 squared values: [8.1160255e-02 1.5683751e-01 1.9388148e-04 4.9947921e-02 5.3757656e-01\n",
            " 8.1706357e-01 6.5382030e-03 6.3362902e-01 8.4135033e-02 9.4004422e-01]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cupy as cp\n",
        "free_mem, total_mem = cp.cuda.Device(0).mem_info\n",
        "print(f\"Free memory: {free_mem / 1024**2:.2f} MB\")\n",
        "print(f\"Total memory: {total_mem / 1024**2:.2f} MB\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-h_kXhsREw_x",
        "outputId": "e1af5b97-998b-4d59-fe59-96037acf8c33"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Free memory: 14596.12 MB\n",
            "Total memory: 15095.06 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cupy as cp\n",
        "import time\n",
        "\n",
        "# Create sample data\n",
        "n_elements = 10000\n",
        "x = cp.random.rand(n_elements, dtype=cp.float32)\n",
        "y = cp.random.rand(n_elements, dtype=cp.float32)\n",
        "\n",
        "# CUDA events for profiling\n",
        "start = cp.cuda.Event()\n",
        "end = cp.cuda.Event()\n",
        "\n",
        "# Launch Kernel with timing\n",
        "start.record()\n",
        "y = x**2 + 2*x + 1  # Example kernel computation\n",
        "end.record()\n",
        "\n",
        "# Synchronize and compute time\n",
        "end.synchronize()\n",
        "time_taken = cp.cuda.get_elapsed_time(start, end)  # Time in milliseconds\n",
        "\n",
        "print(f\"Kernel Execution Time: {time_taken:.6f} ms\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ovwOu9q1FDOi",
        "outputId": "9a768036-64d6-4d49-cfc8-20ee326a6644"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Kernel Execution Time: 538.838989 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Host (CPU) data\n",
        "h_data = cp.random.rand(n_elements, dtype=cp.float32)\n",
        "\n",
        "# Transfer CPU → GPU\n",
        "start.record()\n",
        "d_data = cp.array(h_data)\n",
        "end.record()\n",
        "end.synchronize()\n",
        "cpu_to_gpu_time = cp.cuda.get_elapsed_time(start, end)\n",
        "\n",
        "# Transfer GPU → CPU\n",
        "start.record()\n",
        "h_data_copy = d_data.get()\n",
        "end.record()\n",
        "end.synchronize()\n",
        "gpu_to_cpu_time = cp.cuda.get_elapsed_time(start, end)\n",
        "\n",
        "print(f\"CPU → GPU Transfer Time: {cpu_to_gpu_time:.6f} ms\")\n",
        "print(f\"GPU → CPU Transfer Time: {gpu_to_cpu_time:.6f} ms\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DLjZxGJ6FVzz",
        "outputId": "e06a95af-ce22-492e-fb64-7d89f8cadd8c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU → GPU Transfer Time: 130.486267 ms\n",
            "GPU → CPU Transfer Time: 0.212160 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cupy as cp\n",
        "import cupyx.profiler\n",
        "\n",
        "n_elements = 10000\n",
        "x = cp.random.rand(n_elements, dtype=cp.float32)\n",
        "\n",
        "# Start profiling\n",
        "with cupyx.profiler.time_range('CustomKernel', color_id=0):\n",
        "    y = x**2 + 2*x + 1  # Example computation\n",
        "\n",
        "cp.cuda.Device(0).synchronize()  # Ensure GPU finishes execution\n",
        "print(\"Profiling complete.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gwZNyd8RFYJh",
        "outputId": "ebeea727-7bbd-4ecc-9852-acdd1847ec8a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Profiling complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why Profiling Matters Before Optimization?\n",
        "Profiling helps us identify bottlenecks before making any optimizations. Without it, we might optimize the wrong part of the code, wasting effort without real performance gains.\n",
        "\n",
        "What Profiling Tells Us?\n",
        "Time Spent on Different Operations:\n",
        "\n",
        "How much time is spent on CPU-GPU memory transfers?\n",
        "How much time does the GPU kernel execution take?\n",
        "Memory Usage & Bandwidth:\n",
        "\n",
        "How much global memory is accessed?\n",
        "Is shared memory underutilized?\n",
        "Kernel Performance:\n",
        "\n",
        "Are all GPU threads active?\n",
        "Is register usage limiting performance?\n",
        "Are we experiencing memory access bottlenecks?"
      ],
      "metadata": {
        "id": "zkSZnOBxF4r9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When to Optimize?\n",
        "If memory transfers take a lot of time → Use pinned memory.\n",
        "If GPU utilization is low → Improve parallelism and occupancy.\n",
        "If global memory accesses are slow → Use shared memory."
      ],
      "metadata": {
        "id": "52Q_ni94F75S"
      }
    }
  ]
}